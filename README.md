# BPE_Tokenizer

## Описание

Изначально BPE был представлен как простой алгоритм сжатия данных без потерь. В феврале 1994 года Филипп Гейдж в статье «Новый алгоритм сжатия данных» описал метод, который работает так: самые частотные пары символов заменяются на другой символ, который не встречается в данных, при этом объем используемой памяти снижается с двух байт до одного.


Пример кодировки
[](https://sysblok.ru/wp-content/uploads/2020/11/image2-5.png)![image](https://github.com/AristarkhovZakhar/BPE_Tokenizer/assets/110375755/a165cc54-1b95-4564-8f96-bee139d0feca)

Для задач NLP алгоритм BPE был немного изменен: часто встречающиеся группы символов не заменяются на другой символ, а объединяются в токен и добавляются в словарь. Алгоритм токенизации на основе BPE позволяет моделям узнавать как можно больше слов при ограниченном объеме словаря и выглядит так:

Шаг 0. Создаем словарь.

Шаг 1. Представляем слова из текста как списки букв.

Шаг 2. Считаем количество вхождений каждой пары букв.

Шаг 3. Объединяем самые частотные в токен и добавляем в словарь.

Шаг 4. Повторяем шаг 3 до тех пор, пока не получим словарь заданного размера.

Сегодня схемы токенизации подслов стали нормой в самых продвинутых моделях, включая очень популярное семейство контекстных моделей, таких как BERT, GPT-2, RoBERTa и т. д

# Класс Vocab

| Метод    | Описание   |
| ----------- | ----------- |
|`Vocab(size_t vocab_size_, std::wstring unk_token_, std::wstring eof_token_,std::wstring pad_token_, std::set<std::wstring> extra_tokens_)` | Параметризованный конструктор от размера словаря, unk, pad, eof, extra_tokens|  
|`insert(std::wstring token)` | Операция вставки токена в словарь|
|`merge(std::set<std::wstring> tokens)`| Операция добавления множества токенов в словарь|

# Класс Tokenizer

| Метод    | Описание   |
| ----------- | ----------- |
| `Tokenizer(Vocab vocab)` | Параметризованный конструктор от словаря |  
|`tokenize(std::wstring text)` | Операция разбиения текста на токены |
|`encode(std::vector<std::wstring>)`|Операция получения вектора индексов из вектора токенов|
|`decode(std::vector<int> input_ids)`|Операция получения вектора токенов из вектора индексов|
|`fit(std::vector<std::wstring> &corpus)`|Обучение токенизатора на корпусе текста|
|`file_to_input_ids(std::wstring file_path)`| Операция токенизации текса в файле (передается filepath, создается новый tokenized_filepath, содержимое которого - индексы токенов)|

